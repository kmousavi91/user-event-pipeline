## ğŸ“Š User Event Analytics Pipeline â€“ GCP + DBT + Python

End-to-end data engineering pipeline using **Google Cloud Platform**, **DBT**, and **Python** to process, transform, and validate user interaction logs from a web/mobile application.

---

### ğŸš€ Project Overview

This project simulates a modern data stack used in real-world analytics engineering:

- **Ingestion Layer**: Raw JSON event logs stored in **Google Cloud Storage (GCS)**
- **Processing Layer**: Data loaded into **BigQuery** using Python
- **Transformation Layer**: Data modeled with **DBT** (staging, marts, and Data Vault)
- **Validation Layer**: Python script detects anomalies using rolling averages

---

### ğŸ§± Architecture

```
GCS (raw logs)
   â†“
Python ingestion (google-cloud-storage + bigquery)
   â†“
BigQuery (raw_events table)
   â†“
DBT transformations
   â”œâ”€â”€ staging (stg_raw_events)
   â”œâ”€â”€ marts (daily_events)
   â””â”€â”€ vault (hub_user, hub_event, link_user_event, sat_event_metadata)
   â†“
Python validation script (detect spikes in activity)
```

---

### ğŸ“ Project Structure

```bash
user-event-pipeline/
â”œâ”€â”€ data/                      # Sample JSON files or schema
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_gcs_to_bq.py    # Loads raw JSON into BigQuery
â”‚   â””â”€â”€ validate_event_spikes.py  # Anomaly detection
â”œâ”€â”€ dbt_user_events/           # DBT project (models, tests, config)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â””â”€â”€ vault/
â”œâ”€â”€ event_schema.json          # Schema for BigQuery raw table
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸ› ï¸ Tools Used

- **Google Cloud Storage** â€“ raw data landing zone  
- **BigQuery** â€“ scalable data warehouse  
- **Python** â€“ ingestion & validation scripts  
- **DBT** â€“ transformations, testing, and modeling  
- **Data Vault 2.0** â€“ enterprise data modeling architecture  

---

### ğŸ“Š Data Model Summary

- `raw_events` â€” base table in BigQuery  
- `stg_raw_events` â€” staging view that cleans raw data  
- `daily_events` â€” final table with user/day/device aggregations  
- `hub_user`, `hub_event`, `link_user_event`, `sat_event_metadata` â€” Data Vault models  
- Schema tests ensure data quality and traceability

---

### âœ… How to Run the Project

1. Clone the repo
2. Set up a GCP project and BigQuery dataset
3. Upload sample JSON files to GCS
4. Run:
   ```bash
   python scripts/ingest_gcs_to_bq.py
   dbt run
   dbt test
   python scripts/validate_event_spikes.py
   ```

---

### ğŸ“ˆ Anomaly Detection

A Python script fetches `daily_events` and detects **spikes in user activity** using a 7-day rolling average, printing warnings if anomalies are found.

---

### ğŸ“š Learning Outcomes

- Build scalable, cloud-native data pipelines
- Apply software engineering principles with DBT
- Work with structured and semi-structured data in BigQuery
- Design flexible, auditable models with Data Vault
- Perform basic anomaly detection with Python

---

### ğŸ§‘â€ğŸ’» Author

**Kourosh Mousavi**  
Data Engineer | Physics â†’ AI/Cloud/Data  
Warsaw, Poland  
https://www.linkedin.com/in/kourosh-mohammad-mousavi-895763113/

